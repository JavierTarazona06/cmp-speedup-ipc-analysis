\section{Analyse théorique de cohérence de cache}
\raggedbottom

\subsection{Q1 --- Cohérence de cache et accès mémoire de \texttt{test\_omp}}

\paragraph{Contexte.}
On considère une architecture CMP (plusieurs cœurs) avec un bus partagé et des caches privés par cœur (L1).
Le programme \texttt{test\_omp} réalise une multiplication de matrices \(C = A \times B\) en parallèle via OpenMP.
Dans ce TP, le nombre de threads est couplé au nombre de cœurs : \(nthreads = ncores\).

\paragraph{Nature des accès.}
Dans une multiplication de matrices, \(A\) et \(B\) sont essentiellement \textbf{lus}, tandis que \(C\) est \textbf{écrite}.
Le parallélisme OpenMP répartit typiquement des lignes/blocs de \(C\) entre threads ; idéalement, chaque thread écrit dans une zone disjointe.

\paragraph{Cohérence (intuition type MESI).}
Avec des caches privés, une même ligne mémoire peut être présente dans plusieurs caches.
Un protocole de cohérence garantit qu'une écriture rend visibles les mises à jour :
\begin{itemize}
  \item \textbf{Read miss :} chargement d'une ligne via le bus (depuis mémoire/niveau inférieur), puis réutilisation locale si la ligne reste en cache.
  \item \textbf{Write miss :} obtention de l'exclusivité sur la ligne, impliquant l'\textbf{invalidation} des copies chez les autres cœurs.
  \item \textbf{Writes successifs :} une fois la ligne exclusive, les écritures suivantes sont locales jusqu'à éviction.
\end{itemize}

\paragraph{Effets attendus sur \(A\) et \(B\) (lectures partagées).}
Comme \(A\) et \(B\) sont read-only durant le calcul, les lignes peuvent être \emph{partagées} dans les caches (état \emph{Shared}).
Le trafic bus provient surtout des accès initiaux (miss de froid) et des évictions lorsque la capacité L1 est insuffisante.

\paragraph{Effets attendus sur \(C\) (écritures).}
Pour écrire \(C\), chaque cœur doit obtenir l'exclusivité sur les lignes correspondantes.
Si les zones écrites par thread sont disjointes et alignées, la cohérence génère peu d'interactions.
En revanche, le \textbf{false sharing} peut apparaître : deux threads écrivent des éléments différents mais situés sur la \emph{même ligne de cache},
ce qui provoque une alternance d'invalidations (« ping-pong ») et dégrade fortement le speedup.

\paragraph{Conséquence sur la scalabilité.}
On s'attend à :
\begin{itemize}
  \item un bon gain initial en augmentant \(ncores\) (parallélisme TLP) ;
  \item puis une saturation due à (i) contention mémoire/bus, (ii) surcoûts de cohérence (notamment via \(C\)), (iii) barrières OpenMP,
        et (iv) limites de capacité de cache (miss rate).
\end{itemize}


