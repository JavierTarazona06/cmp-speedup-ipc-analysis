\section{Analyse théorique de cohérence de cache}
\raggedbottom

\subsection{Q1 --- Cohérence de cache et accès mémoire de \texttt{test\_omp}}

\paragraph{Contexte.}
On considère une architecture CMP (plusieurs cœurs) avec un bus partagé et des caches privés par cœur (L1).
Le programme \texttt{test\_omp} réalise une multiplication de matrices \(C = A \times B\) en parallèle via OpenMP.
Dans ce TP, le nombre de threads est couplé au nombre de cœurs : \(nthreads = ncores\).

\paragraph{Nature des accès.}
Dans une multiplication de matrices, \(A\) et \(B\) sont essentiellement \textbf{lus}, tandis que \(C\) est \textbf{écrite}.
Le parallélisme OpenMP répartit typiquement des lignes/blocs de \(C\) entre threads ; idéalement, chaque thread écrit dans une zone disjointe.

\paragraph{Cohérence (intuition type MESI).}
Avec des caches privés, une même ligne mémoire peut être présente dans plusieurs caches.
Un protocole de cohérence garantit qu'une écriture rend visibles les mises à jour :
\begin{itemize}
  \item \textbf{Read miss :} chargement d'une ligne via le bus (depuis mémoire/niveau inférieur), puis réutilisation locale si la ligne reste en cache.
  \item \textbf{Write miss :} obtention de l'exclusivité sur la ligne, impliquant l'\textbf{invalidation} des copies chez les autres cœurs.
  \item \textbf{Writes successifs :} une fois la ligne exclusive, les écritures suivantes sont locales jusqu'à éviction.
\end{itemize}

\paragraph{Effets attendus sur \(A\) et \(B\) (lectures partagées).}
Comme \(A\) et \(B\) sont read-only durant le calcul, les lignes peuvent être \emph{partagées} dans les caches (état \emph{Shared}).
Le trafic bus provient surtout des accès initiaux (miss de froid) et des évictions lorsque la capacité L1 est insuffisante.

\paragraph{Effets attendus sur \(C\) (écritures).}
Pour écrire \(C\), chaque cœur doit obtenir l'exclusivité sur les lignes correspondantes.
Si les zones écrites par thread sont disjointes et alignées, la cohérence génère peu d'interactions.
En revanche, le \textbf{false sharing} peut apparaître : deux threads écrivent des éléments différents mais situés sur la \emph{même ligne de cache},
ce qui provoque une alternance d'invalidations (« ping-pong ») et dégrade fortement le speedup.

\paragraph{Conséquence sur la scalabilité.}
On s'attend à :
\begin{itemize}
  \item un bon gain initial en augmentant \(ncores\) (parallélisme TLP) ;
  \item puis une saturation due à (i) contention mémoire/bus, (ii) surcoûts de cohérence (notamment via \(C\)), (iii) barrières OpenMP,
        et (iv) limites de capacité de cache (miss rate).
\end{itemize}

\section{Paramètres de l'architecture multicoeurs}

\subsection{Q2 --- Paramètres par défaut d'un CPU OoO (BaseO3CPU)}

\paragraph{Remarque.}
Le fichier \texttt{O3CPU.py} sélectionne la variante OoO selon l'ISA.
Les \textbf{paramètres par défaut} se trouvent dans \texttt{src/cpu/o3/BaseO3CPU.py}.

\paragraph{Paramètres OoO retenus (valeurs par défaut).}
Nous reportons ci-dessous des paramètres structurants pour un cœur superscalaire out-of-order
(largeurs du pipeline et taille des structures de renommage/exécution), ainsi que leur interprétation.

\begin{table}[htbp]
\centering
\scriptsize
\caption{Paramètres OoO par défaut (extraits de \texttt{BaseO3CPU.py}) et rôle.}
\label{tab:q2_o3_params}
\begin{tabular}{|l|c|p{4.8cm}|}
\hline
\textbf{Paramètre} & \textbf{Défaut} & \textbf{Rôle / effet attendu} \\
\hline
\texttt{fetchWidth} & 8 & Max instructions fetch/cycle : alimente le pipeline ; utile si le front-end est limitant. \\
\hline
\texttt{decodeWidth} & 8 & Max decode/cycle : limite la capacité de décodage avant renommage/dispatch. \\
\hline
\texttt{issueWidth} & 8 & Max instructions émises/cycle : augmente l'ILP exploitable si dépendances faibles. \\
\hline
\texttt{commitWidth} & 8 & Max instructions retirées/cycle : borne finale du débit d'instructions (retirement). \\
\hline
\texttt{numROBEntries} & 192 & Taille du ROB : fenêtre OoO plus large $\Rightarrow$ meilleure tolérance aux latences (mémoire/branches). \\
\hline
\end{tabular}
\end{table}

\paragraph{Lecture.}
Ces valeurs indiquent un cœur OoO « large » (largeurs à 8) avec une fenêtre OoO conséquente (ROB=192).
Dans le cadre du TP, augmenter la largeur (via \texttt{o3-width}) revient à contraindre/décontraindre ces largeurs effectives,
ce qui agit sur la capacité à exploiter l'ILP à l'intérieur de chaque thread.

\subsection{Q3 --- Valeurs par défaut des caches (Options.py)}

\paragraph{Valeurs par défaut.}
D'après \texttt{configs/common/Options.py}, les paramètres par défaut sont :
\begin{itemize}
  \item \(L1D = 64\ \mathrm{KiB}\), associativité \(2\)-ways ;
  \item \(L1I = 32\ \mathrm{KiB}\), associativité \(2\)-ways ;
  \item \(L2 = 2\ \mathrm{MiB}\), associativité \(8\)-ways ;
  \item taille de ligne = \(64\ \mathrm{B}\).
\end{itemize}

\begin{table}[htbp]
\centering
\scriptsize
\caption{Paramètres de caches par défaut (Options.py).}
\label{tab:q3_cache_defaults}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Niveau} & \textbf{Taille} & \textbf{Associativité} \\
\hline
L1D & 64KiB & 2 \\
\hline
L1I & 32KiB & 2 \\
\hline
L2  & 2MiB  & 8 \\
\hline
Cache line & 64B & -- \\
\hline
\end{tabular}
\end{table}

\paragraph{Commentaire.}
Ces valeurs conditionnent le taux de miss et la pression mémoire.
En multicœur, une augmentation de \(ncores\) augmente le volume de requêtes concurrentes, ce qui rend plus visibles
la contention sur le bus/mémoire et les surcoûts de cohérence.
